<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Koonce, G., &amp; Kelly, M. (October 2011).   Effects of Utilizing Three Instruments for Assessing the Principal Internship</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m41138</md:content-id>
  <md:title>Koonce, G., &amp; Kelly, M. (October 2011).   Effects of Utilizing Three Instruments for Assessing the Principal Internship</md:title>
  <md:abstract>This study was designed to explore principal internship evaluation rigor by examining internship assessment impact on program improvement. Two assessments were added to a university educational leadership preparation program’s internship course requirement. Already in place and utilized for five years is the Principal Internship Mentors Assessment (PIMA). PIMA is an assessment of the intern by the mentoring practitioner (principal) in the field. The University Supervisor Assessment (USA) and the Intern’s Self-Assessment (ISA) were added for a more comprehensive evaluation of the intern. Fifty-four student intern scores on the PIMA, USA, and ISA were examined. The null- hypothesis was rejected at the .05 level that there will be no significant difference on the means of the three internship assessments. Three program improvement recommendations are made.</md:abstract>
  <md:uuid>1f6a566e-f661-4936-8e88-3dbed9164da3</md:uuid>
</metadata>

<content>
    
    
    
    <para id="eip-941"><media id="id27091673" alt=""><image src="../../media/ncpealogo.gif" mime-type="image/gif"/></media>


</para><section id="eip-461"><title>NCPEA Education Leadership Review: Portland Conference Special Edition, Volume 12, Number 3 (October 2011)</title><note id="eip-187">This manuscript has been peer-reviewed, accepted, and endorsed by the National Council of Professors of Educational Administration (NCPEA) as a significant contribution to the scholarship and practice of education administration. In addition to publication in the Connexions Content Commons, this module is published in the <link url="http://www.ncpeapublications.org"><emphasis effect="italics"> Education Leadership Review: Special Portland Conference Issue (October 2011)</emphasis>
	</link>, ISSN 1532-0723. Formatted and edited in Connexions by Theodore Creighton and Brad Bizzell, Virginia Tech and Janet Tareilo, Stephen F. Austin State University.
</note></section><section id="id1165961402509">
      <title>Introduction</title>
      <para id="id1165970468388">Accrediting organizations and state program approval processes are focusing on field experiences for principal candidates being prepared at our nation’s universities. Effective principal internships are designed as a culminating curriculum component of a comprehensive standards based program. In order to evaluate the effectiveness of principal internships faculty must assess outcomes and use the data for program improvement.</para>
    </section>
    <section id="id1165968309943">
      <title>Purpose</title>
      <para id="id1165963531796">The principal internship is typically the last course a student takes in their preparation program. “The internship is considered by many practicing principals to be the most valuable component of their preparation program, as schools provide the laboratories where the connection between educational leadership theory with practice and application can best be made” (Bost, 2009, p. 15). In an attempt to develop a greater understanding of the relationship of internship evaluation on program outcomes and program improvement, a correlation study was conducted. </para>
      <para id="id1165978540256"> The purpose of this study was to examine three principal internship assessments for educational leadership preparation. This study provides quantitative measures for evaluating the principal internship. The three assessments examined are the Principal Internship Mentor’s Assessment (PIMA), the University Supervisors Assessment (USA), and the Intern’s Self-Assessment (ISA). PIMA, USA, and ISA outcomes were and results examined for program improvement.</para>
    </section>
    
    <section id="id1165972326078">
      <title>Rationale and Significance of the Study</title>
      <para id="id1165969010377"> A review of the literature suggests few studies have assessed the outcomes for evaluating the principal internship and the impact of evaluation on program improvement. This investigation will utilize outcome data for improving programs.</para>
    </section>
    <section id="id1165972889916">
      <title>Research Questions</title>
      <para id="id1165959381981"> Two research questions have emerged that focus on outcomes for evaluating principal internships:</para>
      
      <list id="eip-692" list-type="bulleted" bullet-style="none"><item>RQI: Are there significant differences in assessment outcomes for the PIMA, USA, and ISA?</item>
	<item>RQ2: If there are significant differences in assessment outcomes for the PIMA, USA, and ISA, what are they and what meaning do they have for program improvement?</item>
	</list>
    </section>
    <section id="id1165985256201">
      <title>Null-Hypothesis </title>
      <para id="id1165969457076"> There will be no significant difference in the means of three assessments, the Principal Internship Mentor’s Assessment, the University Supervisor Assessment, and the Intern Self-Assessment for a university principal preparation program internship.</para>
    </section>
    <section id="id1165989769950">
      <title>Literature Review</title>
      <para id="id1165970237033"> There appears to be as many internship assessments as there are university principal preparation programs. In addition, states vary greatly in their principal internship requirements and accrediting agencies require that assessment data for the principal internship be analyzed for program improvement (TEAC, 2010). </para>
      <section id="id1165961581847">
        <title>Assessment of Principal Interns</title>
        <para id="id1165969760547">Individual assessments from principal mentors in the field are common. Some programs use only a principal mentor’s assessment or university supervisor’s assessment while others utilize an intern’s self-assessment. Some programs use a combination of two of the assessments just noted, and a few use all three in their evaluation of the internship.</para>
        <para id="id1165961399203"> Several programs use only a mentor’s assessment in evaluating the internship. The University of Illinois (2008) utilizes an instrument titled <emphasis effect="italics">Principal Preparation Program Redesign Internship Assessment Scoring Rubric</emphasis>. This instrument displays three assessment domains that include focus areas within each domain and a Likert-type rating scale; the assessment is completed by the mentoring principal in the field.</para>
        <para id="id1165995178371"> Washington College (2011) in Chestertown, Maryland utilizes the <emphasis effect="italics">Site Supervisor Final Intern Assessment Form</emphasis> that is completed by the mentoring principal in the field. The instrument has a Likert-type scale with assessment ratings ranging from 1 (Unsatisfactory) to 5 (Exceptional). Categories for scoring are not aligned with the ISLLC standards and appear to be categories for feedback designed by program faculty. </para>
        <para id="id1165972847206">Other programs directly link the ISLLC Standards to the assessments. Indiana State (2011) utilizes the <emphasis effect="italics">Administrative Intern Evaluation</emphasis> form. The form includes three levels of measurement; “Exceeds Expectations, Meet Expectations, or Does Not Meet Expectations” (p. 29). Following the evaluation descriptions, a phrase from each of the six ISLLC standards is listed with three boxes where the evaluator can place a check indicating the score. </para>
        <para id="id1165972378829"> An instrument utilized by the Danforth Educational Leadership Program at the University of Washington (2005), located in Seattle, is designed using the ISLLC standards. Each ISLLC standard is stated followed by four to six elements defining the standards. There are four performance ratings for the mentoring principal to select. The ratings are designed as a check-off for the entire standard, not for each element. </para>
        <para id="id1165959249882"> The University of Oklahoma (2010) lists the six ISLLC standards followed by a four point Likert-type scale under each in their <emphasis effect="italics">Supervising Administrator’s Assessment of Intern</emphasis>. No rubric was found for the instrument but a university supervisor prepares a formal evaluation of the intern using the <emphasis effect="italics">Principal Internship Scoring Criteria</emphasis>, a six item instrument with three rating scales.</para>
        <para id="id1165959147040"> At Seattle Pacific University (2011), both the principal and the intern rate the post-internship skill level on each of the ISLLC standards. The university supervisor can make comments on the <emphasis effect="italics">Principal/ Program Administrator Intern Summative Evaluation</emphasis> form, but identifies no separate measurements indicating performance level of the intern. Even though all three, the mentoring principal, the intern, and the university supervisor have input, only one instrument is used to collect data. </para>
        <para id="id1165974585018"> The University of Wyoming (2011) incorporates two identical instruments, except for rating scale terminology, one titled the <emphasis effect="italics">Intern Self-Evaluation </emphasis>- <emphasis effect="italics">Pre and Post</emphasis>, and the <emphasis effect="italics">Mentor/Supervisor Evaluation for Internship</emphasis>. There are 40 items describing leadership knowledge, skills, and dispositions. The scoring scales for each ISLLC standard are the only differences. </para>
        <para id="id4665101"> Baker University (2011) utilizes three assessments for the principal internship. The three are most similar to the PIMA, USA, and ISA. Baker University’s <emphasis effect="italics">Administrative Mentor’s Evaluation of Field Experiences</emphasis>; <emphasis effect="italics">Supervisor’s Evaluation of Field Experiences</emphasis>, and <emphasis effect="italics">Intern’s </emphasis><emphasis effect="italics">Self-Evaluation of Field Experiences</emphasis> are derived from the six ISLLC standards and includes a 4 point Likert-type scale.</para>
        <para id="id1165972727426">It is widely known that principal preparation programs include an internship. In Virginia it is a mandate for approved programs (VDOE, 2011). The sixty-two (62) university programs reviewed in the literature produced a small number and variety of instruments but no formal analysis of outcomes or review of the outcomes for program improvement. </para>
      </section>
    </section>
    <section id="id1165968853022">
      <title>Methodology</title>
      <section id="id1165999270280">
        <title>Instruments</title>
        <para id="id1165963696555"> Three assessments were devised for collecting data in this study, the PIMA, the USA, and the ISA. Each is a 24-item Likert-type scale instrument derived from the ISLLC standards. There are four items per standard, and six standards, with each item being rated on a 5-point Likert-type scale that ranges from: <emphasis effect="italics">fails to address/no evidence of knowledge, understanding and/or application</emphasis> to <emphasis effect="italics">very specific/convincing evidence of knowledge, understanding, and/or application</emphasis>.</para>
        <para id="id1165969735882">There are four items per standard taken from <emphasis effect="italics">Components of Professional Practice for School Leaders</emphasis> (Hessel &amp; Holloway, 2002). The rating form reflects the 24 Components of Professional Practice for School Leaders derived from the ISLLC standards (Hessel &amp; Holloway, 2002, p. 27). The 24 components are:</para>
        
        <list id="eip-992" list-type="labeled-item"><title>Standard 1: The Vision of Learning</title><item>1a. Developing the Vision
</item>
	<item>1b. Communicating the Vision
</item>
	<item>1c. Implementing the Vision
</item>
	<item>1d. Monitoring and Evaluating the Vision
</item></list><list id="eip-901" list-type="labeled-item"><title>Standard 2: The Culture of Teaching and Learning</title><item>2a. Valuing Students and Staff
</item>
	<item>2b. Developing and Sustaining the Culture
</item>
	<item>2c. Ensuring an Inclusive Culture
</item>

<item>2d. Monitoring and Evaluating the Culture
</item>
</list><list id="eip-601" list-type="labeled-item"><title>Standard 3: The Management of Learning</title><item>3a. Making Management Decisions
</item>
	<item>3b. Developing Procedures to Ensure Successful Teaching and Learning
</item>
	<item>3c. Allocating Resources to Ensure Successful Teaching and Learning
</item>
<item>3d. Creating a Safe, Healthy Environment to Ensure Successful Teaching and Learning</item>

</list><list id="eip-17" list-type="labeled-item"><title>Standard 4: Relationships with the Broader Community to Foster Learning</title><item>4a. Understanding Community Needs
</item>
	<item>4b. Involving Members of the Community</item>
	<item>4c. Providing Opportunities for the Community and School to Serve Each Other
</item>
<item>4d. Understanding and Valuing Diversity
</item>

 
</list><list id="eip-241" list-type="labeled-item"><title>Standard 5: Integrity, Fairness, and Ethics in Learning</title><item>5a. Demonstrating a Personal and Professional Code of Ethics
</item>
	<item>5b. Understanding One’s Impact on the School and Community
</item>
	<item>5c. Respecting the Rights and Dignity of All
</item>
<item>5d. Inspiring Integrity and Ethical Behavior in Others
</item>
</list><list id="eip-382" list-type="labeled-item"><title>Standard 6: The Political, Social, Economic, Legal, and Cultural Context of Learning</title><item>6a. Operating Schools on Behalf of Students and Families
</item>
	<item>6b. Communicating Changes in Environment to Stakeholders
</item>
	<item>6c. Working Within Policies, Laws, and Regulations
</item>


<item>6d. Communicating with Decision-Makers Outside the School Community</item>
</list><para id="id1165961604207">The instrument was piloted in the 2006-2007 academic year. The Educational Focus Group (Cannizzaro, 2007) provided feedback on the instrument and confirmed its content validity. To address inter-rater reliability, sets of two raters used the instrument and discussed the outcomes. The four teams of practitioners rated the PIMA similarly (Cannizzaro, 2007). Koonce and Causey (2011) found consistency of variance within the PIMA and its constituent items. For each standard the constituent items correlate robustly with the standard overall. Pearson’s <emphasis effect="italics">r</emphasis> values ranged between .779 and .980. All findings fell within the .01 level of significance. </para>
        <para id="id1165959930482">Upon establishing the validity and reliability of the PIMA, the USA and ISA assessments were subsequently designed using the PIMA as a model. Other than the title and lead in information, the assessments are identical with the same Likert-type ratings. </para>
      </section>
      <section id="id1165959902714">
        <title>Participants</title>
        <para id="id1165973571097"> Fifty-four (54) educational leadership internship completers were studied in this research from one university Educational Leadership Preparation Program in southeast Virginia. The participants were uncompensated and were not interviewed, tested or surveyed beyond the normal program requirements. All were licensed and experienced educators (minimum three years teaching experience) prior to participating in the internship. The participant group makes up a purposeful sample consisting of all students that completed the internship between September 2009 and May 2011. </para>
      </section>
      <section id="id1165959213566">
        <title>Data Collection and Statistical Analysis</title>
        <para id="id1165972586950"> Data were collected online via Survey Monkey through the mentoring principals’ completion of the Principal Internship Mentor’s Assessment (PIMA), the university supervisor completing the University Supervisor’s Assessment (USA), and the intern completing the Intern Self-Assessment (ISA) for each semester and each student completing an internship (Figure 1). Data were then organized into an Excel spreadsheet entered into the Statistical Package for the Social Sciences (SPSS). </para>
        <para id="id1165968272030"><title>Figure 1. Three-way Principal Internship Assessment</title><link url="figure1_koonce.png/image" window="new">
		<media id="media1" alt="">
			<image mime-type="image/png" src="../../media/figure 1.png" id="figure1"/>
		</media>
	</link></para>
        
        <para id="id1165973179414">A Repeated Measures Analysis Of Variance (ANOVA) was performed to examine the null-hypothesis of no difference between the means (Lund, 2010a) for the three internship assessments under each of the six ISLLC standards. Prior to conducting each Repeated Measures ANOVA, researchers first determined sphericity using Mauchly's Test of Sphericity. Sphericity is the condition where the covariances between all levels of the within subjects variable are equal (Lund, 2010b). Because ANOVAs with repeated measures are particularly susceptible to the violation of the assumption of sphericity with violation causing the test to become too liberal (i.e., an increase in the Type I error rate), sphericity must be tested in order to determine the proper repeated measures test (i.e., univariate vs. multivariate; Lund, 2010b). </para>
        <para id="id1165972930309">When the statistical analysis determined that sphericity could be assumed, a univariate test was used to analyze the within subjects effect; if sphericity could not be assumed, a multivariate test using Wilks’ Lambda was used. If the appropriate within subjects test revealed a difference significant at the 0.05 level, paired samples t-<emphasis effect="italics"/>tests were used as a post hoc analysis. To reduce the possibility of a Type I error, the Bonferroni correction to α was implemented (Simon, 2008).</para>
      </section>
    </section>
    <section id="id1165971931399">
      <title>Findings</title>
      <para id="id1165970975063"> Through the statistical analysis of the data, it was determined that there were no significant differences between the means in three of the six ISLLC standards with regard to the assessments completed by the mentors, university supervisors, and the students themselves. For these three standards (ISLLC 1, 3 and 6), the researchers failed to reject the null hypothesis that there will be no significant difference in the means of three assessments. Table 1 provides the descriptive statistics for all responses to items under ISLLC standards 1-6.</para>
      <section id="id3637284">
        <title>General Linear Model comparing responses for ISLLC Standard 2</title>
        <para id="id1165960391618">In the analysis of ISLLC Standard 2 responses, results of Mauchly’s Test of Sphericity on the covariance matrix indicated a significant difference of less than 0.05 (<emphasis effect="italics">p =</emphasis> .000). As such, sphericity could not be assumed for this standard and multivariate testing was again needed. The Wilks’ Lambda multivariate test revealed a level of significance less than 0.05 (p = .001). As such the null hypothesis was rejected and paired t- tests were then used as a post hoc analysis. </para>
        <table id="id2350409" summary="Descriptive Statistics of ISLLC Standards Means"><title><emphasis effect="italics">Descriptive Statistics of ISLLC Standards Means</emphasis></title>
<tgroup cols="4"><colspec colnum="1" colname="c1"/>
	<colspec colnum="2" colname="c2"/>
	<colspec colnum="3" colname="c3"/>
	<colspec colnum="4" colname="c4"/>
	<tbody>
		<row>
			<entry>ISLLC 1</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.1806</entry>
			<entry>.82761</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.4028</entry>
			<entry>.60738</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.4861</entry>
			<entry>.61892</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>ISLLC 2</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.4676</entry>
			<entry>.55806</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.7130</entry>
			<entry>.38056</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.5000</entry>
			<entry>.67642</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>ISLLC 3</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.4259</entry>
			<entry>.62122</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.5278</entry>
			<entry>.47001</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.4213</entry>
			<entry>.70681</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>ISLLC 4</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.3611</entry>
			<entry>.65816</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.7176</entry>
			<entry>.39468</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.4213</entry>
			<entry>.70681</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>ISLLC 5</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.7264</entry>
			<entry>.45006</entry>
			<entry>53</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.8349</entry>
			<entry>.33233</entry>
			<entry>53</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.5283</entry>
			<entry>.75584</entry>
			<entry>53</entry>
		</row>
		<row>
			<entry>ISLLC 6</entry>
			<entry>Mean</entry>
			<entry>Std. Deviation</entry>
			<entry>N</entry>
		</row>
		<row>
			<entry>Mentor</entry>
			<entry>3.1759</entry>
			<entry>.85593</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Univ. Supervisor</entry>
			<entry>3.3102</entry>
			<entry>.49272</entry>
			<entry>54</entry>
		</row>
		<row>
			<entry>Student</entry>
			<entry>3.1250</entry>
			<entry>.92496</entry>
			<entry>54</entry>
		</row>
	</tbody>

</tgroup>
</table>
        
        <para id="id1165973182077">Three paired samples t-tests (Table 2) were used to make post hoc comparisons between the means. The Bonferroni correction to α was applied, using a 0.017 level of significance. The first paired samples t-test indicated that there was a significant difference in the scores for PIMA ISLLC 2 responses (M = 3.47, <emphasis effect="italics">p </emphasis>= 0.001) and USA ISLLC 1 responses (M = 3.71, <emphasis effect="italics">p </emphasis>= 0.023). There was no significant difference in the scores for PIMA ISLLC 2 responses (M = 3.47, <emphasis effect="italics">p </emphasis>= 0.783) and ISA ISLLC 2 responses (M = 3.50, <emphasis effect="italics">p </emphasis>= 0.783) or for USA responses (M = 3.71, <emphasis effect="italics">p </emphasis>= 0.033) and ISA responses (M = 3.50, <emphasis effect="italics">p </emphasis>= 0.033). The post hoc analysis revealed a significant difference only in responses between the mentors and the university supervisors under ISLLC Standard 2.</para>
        
        
        <table id="id1165967307148" summary="Paired Samples Test for ISLLC 2"><title><emphasis effect="italics">Paired Samples Test for ISLLC 2</emphasis></title>
<tgroup cols="5"><colspec colnum="1" colname="c1"/>
	<colspec colnum="2" colname="c2"/>
	<colspec colnum="3" colname="c3"/>
	<colspec colnum="4" colname="c4"/>
	<colspec colnum="5" colname="c5"/>
	<tbody>
		<row>
			<entry>Pairing </entry>
			<entry>Mean </entry>
			<entry>Std. Dev. </entry>
			<entry>Std. Error </entry>
			<entry>Sig (2-tailed) </entry>
		</row>
		<row>
			<entry>Mentor – USA </entry>
			<entry>-.24537 </entry>
			<entry>.53529 </entry>
			<entry>.07284 </entry>
			<entry>.001* </entry>
		</row>
		<row>
			<entry>Mentor-Student </entry>
			<entry>-.03241 </entry>
			<entry>.85925 </entry>
			<entry>.11693 </entry>
			<entry>.783 </entry>
		</row>
		<row>
			<entry>USA - Student </entry>
			<entry>.21296 </entry>
			<entry>.71442 </entry>
			<entry>.09722 </entry>
			<entry>.033 </entry>
		</row>
	</tbody>

</tgroup>
</table>
      </section>
      <section id="id3623076">
        <title>General Linear Model comparing responses for ISLLC Standard 4</title>
        <para id="id1165968637847">The analysis of ISLLC Standard 4 responses did reveal that sphericity of the covariance matrix could be assumed through Mauchly’s Test of Sphericity. As such, a univariate test was used to analyze the within subjects effect and test the ANOVA null hypothesis. Results of the univariate test indicated a significant difference of less than .05 (<emphasis effect="italics">p </emphasis>= .002). The null hypothesis was rejected and paired t-tests were run as a post hoc analysis. </para>
        <para id="id1165970492989">The post hoc analysis of ISLLC standard 4 (Table 3) revealed a significant difference in responses between the mentors (M = 3.36, <emphasis effect="italics">p </emphasis>= 0.000) and the university supervisors (M = 3.72, <emphasis effect="italics">p </emphasis>= 0.000), as well as a significant difference in responses between the university supervisors (M = 3.72, <emphasis effect="italics">p </emphasis>= 0.012) and the students (M = 3.42, <emphasis effect="italics">p </emphasis>= 0.012), with Bonferroni correction applied to α at 0.017. No significant difference was noted in the scores for mentor responses (M = 3.36, <emphasis effect="italics">p </emphasis>= 0.591) and student self assessment responses (M = 3.42, <emphasis effect="italics">p </emphasis>= 0.591). </para>
        
        
        <table id="id1165972803049" summary="Paired Samples Test for ISLLC 4"><title><emphasis effect="italics">Paired Samples Test for ISLLC 4</emphasis></title>
<tgroup cols="5"><colspec colnum="1" colname="c1"/>
	<colspec colnum="2" colname="c2"/>
	<colspec colnum="3" colname="c3"/>
	<colspec colnum="4" colname="c4"/>
	<colspec colnum="5" colname="c5"/>
	<tbody>
		<row>
			<entry>Pairing </entry>
			<entry>Mean </entry>
			<entry>Std. Dev. </entry>
			<entry>Std. Error </entry>
			<entry>
				<emphasis effect="bold">Sig (2-tailed) </emphasis>
			</entry>
		</row>
		<row>
			<entry>Mentor – USA </entry>
			<entry>-.35648 </entry>
			<entry>.67223 </entry>
			<entry>.09148 </entry>
			<entry>.000* </entry>
		</row>
		<row>
			<entry>Mentor-Student </entry>
			<entry>-.06019 </entry>
			<entry>.81833 </entry>
			<entry>.11136 </entry>
			<entry>.591 </entry>
		</row>
		<row>
			<entry>USA - Student </entry>
			<entry>.29630 </entry>
			<entry>.83986 </entry>
			<entry>.11429 </entry>
			<entry>.012* </entry>
		</row>
	</tbody>

</tgroup>
</table>
      </section>
      <section id="id1165959487461">
        <title>General Linear Model comparing responses for ISLLC Standard 5</title>
        <para id="id1165960790650">In the analysis of ISLLC Standard 5 responses, results of Mauchly’s Test of Sphericity on the covariance matrix indicated a significant difference of less than 0.05 (<emphasis effect="italics">p =</emphasis> .001). Sphericity could not be assumed for this standard either, and multivariate testing was once again needed. Results from the multivariate test using Wilks’ Lambda under ISLLC Standard 5 revealed a level of significance less than 0.05 (p = .017) so the null hypothesis was rejected and paired t- tests were then used as a post hoc analysis.</para>
        <para id="id1165972690907">The paired samples tests comparison of means between mentor responses (M = 3.73, <emphasis effect="italics">p </emphasis>= 0.142) and university supervisor responses (M = 3.84, <emphasis effect="italics">p </emphasis>= 0.142) as well as between mentors (M = 3.73, <emphasis effect="italics">p </emphasis>= 0.100) and student self assessments (M = 3.53, <emphasis effect="italics">p </emphasis>= 0.100) for ISLLC standard 5 demonstrated no significant differences. The responses did reveal a significant difference in the scores for university supervisors (M = 3.84, <emphasis effect="italics">p </emphasis>= 0.006) and student self assessments (M = 3.53, <emphasis effect="italics">p </emphasis>= 0.006). </para>
        
        
        <table id="id1165961532170" summary="Paired Samples Test for ISLLC 5"><title><emphasis effect="italics">Paired Samples Test for ISLLC 5</emphasis></title>
<tgroup cols="5"><colspec colnum="1" colname="c1"/>
	<colspec colnum="2" colname="c2"/>
	<colspec colnum="3" colname="c3"/>
	<colspec colnum="4" colname="c4"/>
	<colspec colnum="5" colname="c5"/>
	<tbody>
		<row>
			<entry>Pairing </entry>
			<entry>Mean </entry>
			<entry>Std. Dev. </entry>
			<entry>Std. Error </entry>
			<entry>Sig (2-tailed) </entry>
		</row>
		<row>
			<entry>Mentor – USA </entry>
			<entry>-.10648 </entry>
			<entry>.52441 </entry>
			<entry>.07136 </entry>
			<entry>.142 </entry>
		</row>
		<row>
			<entry>Mentor-Student </entry>
			<entry>.19811 </entry>
			<entry>.86096 </entry>
			<entry>.11826 </entry>
			<entry>.100 </entry>
		</row>
		<row>
			<entry>USA - Student </entry>
			<entry>.30660 </entry>
			<entry>.78392 </entry>
			<entry>.10768 </entry>
			<entry>.006* </entry>
		</row>
	</tbody>

</tgroup>
</table>
      </section>
      <section id="id1165973162795">
        <title>Summary</title>
        <para id="id7039183"> The analysis of the variance for each of the three assessments (PIMA, USA and ISA) demonstrated no significant differences in the means for ISLLC Standards 1, 3 or 6. There were significant differences in responses for ISLLC Standards 2, 4 and 5. </para>
        <para id="id1165963777633">In each of the cases where significant means were identified, the responses of the university supervisors were involved. ISLLC standard 2 responses showed differences between the mentor responses and the university supervisors. Standard 4 revealed differences between mentor responses and the university supervisors, as well as student responses and the university supervisors. ISLLC 5 responses demonstrated significant differences between the student responses and the university supervisors as well. </para>
      </section>
    </section>
    <section id="id1165967445038">
      <title>Conclusions </title>
      <para id="id1165976860321">From the results of this research, it is clear that both the mentoring principals and the interns themselves were consistent with regards to the means in their scoring on the internship assessments. Though individual results may have varied, as a group the differences in mean scores between the two groups were statistically insignificant. The university supervisors on the other hand were consistent in their scoring in three of the six ISLLC standards, but showed significant differences in their mean scores in half of the standards compared to either the mentor scores or the individual intern scores. </para>
      <para id="id1165961410454">The implications of the data review have generated three recommendations for program improvement from the researchers. The first is for the program to design a common rubric for completing the PIMA, USA, and ISA. A rubric with accompanying information on it’s use would hopefully prevent divergent scores on the three assessments. Second, additional validity and reliability measures need to coincide with the implementation of a scoring rubric. Third, the data results from this study should be correlated with other program performance indicators, such as scores on the School Leaders Licensure Assessment (SLLA). </para>
      <para id="id5895207">Further study could be conducted in the effectiveness of the three assessments after the subsequent creation and implementation of the rubric to coincide with the assessments. This study could take two forms: first, it could repeat the process used in this study again to see if the means between the three groups still vary after the implementation of a rubric; second, it could compare means from the first test group who were evaluated without a rubric, and the latest group who did make use of a scoring rubric.</para>
      <para id="id1165972610556">A final area for further research relates directly to the professors who supervise the internship program. Because the university supervisors were involved in all of the areas of significant difference in this study, it is imperative that the researchers delve deeper into the cause. If further research can determine the reasons behind the differences in scoring, the program will then be able to better evaluate its students. </para>
    </section>
    <section id="id1165963570574">
      <title>References</title>
      <list id="id1165968267706" list-type="bulleted" bullet-style="none"><item>Baker University. (2011). <emphasis effect="italics">Baker University School of Education Directed Field Experience I &amp; II Handbook for the Doctor of Education in Educational Leadership</emphasis>. Retrieved from http://www.bakeru.edu/images/pdf/SOE/policy_handbook_EdD_DFE_0809.pdf</item>
	<item>Bost, D. N. (2009). <emphasis effect="italics">An examination of the effect of state level policy in changing professional preparation: A case study of Virginia principal preparation programs and regulatory implementation</emphasis>. Unpublished doctoral dissertation, Virginia Commonwealth University.</item>
		<item>Cannizzaro, S. V. (2007). <emphasis effect="italics">Executive summary: Focus group of practitioners in educational leadership</emphasis>. Paper presented at Regent University, School of Education, Virginia Beach, VA.</item>
		<item>Hessel, K., &amp; Holloway. (2002). <emphasis effect="italics">A framework for school leaders license; Linking the ISLLC standards to practice</emphasis>. Princeton, NJ: Educational Testing Services.</item>
	<item>Indiana State University. (2011). <emphasis effect="italics">Principal Internship Handbook: Principal Preparation Program</emphasis>. Retrieved from http://coe.indstate.edu/elaf/docs/PRINCIPALINTERNHANDBOOK2010-2011.pdf</item>
	<item>Koonce, G. L., &amp; Causey, R. (2011). Standards-based assessment for principal interns. <emphasis effect="italics">The International Journal of Educational Leadership Preparation, 6</emphasis>(1).</item>
	<item>Lund, A., &amp; Lund, M. (2010a). <emphasis effect="italics">ANOVA with repeated measures using SPSS</emphasis>. Retrieved from http://statistics.laerd.com/spss-tutorials/one-way-anova-repeated-measures-using-spss-statistics.php </item>
	<item>Lund, A., &amp; Lund, M. (2010b). <emphasis effect="italics">Sphericity</emphasis>. Retrieved from http://statistics.laerd.com/statistical-guides/sphericity-statistical-guide.php </item>
			<item>Seattle Pacific University. (2011). <emphasis effect="italics">Educational Leadership Principal/Program Administrator Intern Summative Evaluation</emphasis>. Retrieved from http://www.spu.edu/depts/soe/academics/certifications/documents/PrincipalInternmanual10-11finalvrs.doc</item>
	<item>Simon, S. (2008). <emphasis effect="italics">Bonferroni correction.</emphasis> Retrieved from http://www.childrensmercy.org/stats/ask/bonferroni.asp </item>
	<item>Teacher Education Accreditation Council (TEAC). (2010). <emphasis effect="italics">Guide to accreditation</emphasis>. Washington, DC.</item>
	<item>The University of Oklahoma. (2010). <emphasis effect="italics">EACS 5920 Principal Internship Portfolio &amp; Syllabus</emphasis>. Retrieved from http://education.ou.edu/eacs_files/documents/EACS_5920_Principal_Internship.pdf</item>
	<item>The University of Wyoming. (2011). <emphasis effect="italics">Educational Leadership Program, Department of Professional Studies: University of Wyoming Intern Self Evaluation-School Level (Pre); </emphasis><emphasis effect="italics">University of Wyoming Intern Self Evaluation-School Level (Post); University of Wyoming Mentor/Supervisor Evaluation for Internship</emphasis>. Retrieved from http://www.uwyo.edu/edleadsupport/Prin%20Internship/Principal_Internship_Packet.pdf </item>
	<item>University of Illinois. (2008).<emphasis effect="italics"> Principal preparation program redesign: Internship assessment scoring rubric</emphasis>. Retrieved from http://www.illinoisschoolleader.org/principal_preparation/documents/INTERNSHIPASSESSMENTSCORINGRUBRICS-1-2-3DRAFTSwConfCalwEdits-0629091.pdf </item>
	<item>University of Washington. (2005).<emphasis effect="italics"> Danforth Educational Leadership Program: Internship Evaluation by Mentor</emphasis>. Retrieved from http://depts.washington.edu/k12admin/danforth/mentors/resources.html</item>
	<item>Virginia Department of Education (VDOE). (2011). <emphasis effect="italics">Licensure regulations for school personnel</emphasis>. State Board of Education 8VAC-20-22-10et. Seq. Richmond, VA</item>
	<item>Washington College. (2011). <emphasis effect="italics">Site Supervisor final intern assessment form: Washington College Internship Program</emphasis>. Retrieved from http://internships.washcoll.edu/pdf/academicinternships_finalemployerevaluation.pdf</item>
</list>
    </section>
  </content>
</document>